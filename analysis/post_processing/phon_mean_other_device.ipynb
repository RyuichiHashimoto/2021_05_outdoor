{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5664b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "from lib.io import load_pickle_data\n",
    "from lib.noglobal import noglobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c345927",
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal()\n",
    "def apply_gauss_smoothing(df,params):\n",
    "    \n",
    "    SZ_1 = params[\"sz_1\"]\n",
    "    SZ_2 = params[\"sz_2\"]\n",
    "    SZ_CRIT = params['sz_crit']    \n",
    "    \n",
    "    \n",
    "    pd_list = [];\n",
    "    for key,each_df in df.groupby(\"phone\"):\n",
    "        tmp_df = each_df.copy();\n",
    "        \n",
    "        data = each_df[[\"latDeg\",\"lngDeg\"]].to_numpy();\n",
    "        \n",
    "        lat_g1 = gaussian_filter1d(data[:, 0], np.sqrt(SZ_1))\n",
    "        lon_g1 = gaussian_filter1d(data[:, 1], np.sqrt(SZ_1))\n",
    "        lat_g2 = gaussian_filter1d(data[:, 0], np.sqrt(SZ_2))\n",
    "        lon_g2 = gaussian_filter1d(data[:, 1], np.sqrt(SZ_2))\n",
    "        \n",
    "        lat_dif = data[1:,0] - data[:-1,0]\n",
    "        lon_dif = data[1:,1] - data[:-1,1]\n",
    "        \n",
    "        lat_crit = np.append(np.abs(gaussian_filter1d(lat_dif, np.sqrt(SZ_CRIT)) / (1e-9 + gaussian_filter1d(np.abs(lat_dif), np.sqrt(SZ_CRIT)))),[0])\n",
    "        lon_crit = np.append(np.abs(gaussian_filter1d(lon_dif, np.sqrt(SZ_CRIT)) / (1e-9 + gaussian_filter1d(np.abs(lon_dif), np.sqrt(SZ_CRIT)))),[0])           \n",
    "            \n",
    "        tmp_df[\"latDeg\"] = lat_g1 * lat_crit + lat_g2 * (1.0 - lat_crit)\n",
    "        tmp_df['lngDeg'] = lon_g1 * lon_crit + lon_g2 * (1.0 - lon_crit)    \n",
    "        \n",
    "        pd_list.append(tmp_df);\n",
    "                       \n",
    "    \n",
    "    return pd.concat(pd_list).sort_index()\n",
    "    \n",
    "def mean_with_other_phones(df):\n",
    "    collections_list = df[['collectionName']].drop_duplicates().to_numpy()\n",
    "\n",
    "    for collection in collections_list:\n",
    "        phone_list = df[df['collectionName'].to_list() == collection][['phoneName']].drop_duplicates().to_numpy()\n",
    "\n",
    "        phone_data = {}\n",
    "        corrections = {}\n",
    "        for phone in phone_list:\n",
    "            cond = np.logical_and(df['collectionName'] == collection[0], df['phoneName'] == phone[0]).to_list()\n",
    "            phone_data[phone[0]] = df[cond][['millisSinceGpsEpoch', 'latDeg', 'lngDeg']].to_numpy()\n",
    "\n",
    "        for current in phone_data:\n",
    "            correction = np.ones(phone_data[current].shape, dtype=np.float)\n",
    "            correction[:,1:] = phone_data[current][:,1:]\n",
    "            \n",
    "            # Telephones data don't complitely match by time, so - interpolate.\n",
    "            for other in phone_data:\n",
    "                if other == current:\n",
    "                    continue\n",
    "\n",
    "                loc = interp1d(phone_data[other][:,0], \n",
    "                               phone_data[other][:,1:], \n",
    "                               axis=0, \n",
    "                               kind='linear', \n",
    "                               copy=False, \n",
    "                               bounds_error=None, \n",
    "                               fill_value='extrapolate', \n",
    "                               assume_sorted=True)\n",
    "                \n",
    "                start_idx = 0\n",
    "                stop_idx = 0\n",
    "                for idx, val in enumerate(phone_data[current][:,0]):\n",
    "                    if val < phone_data[other][0,0]:\n",
    "                        start_idx = idx\n",
    "                    if val < phone_data[other][-1,0]:\n",
    "                        stop_idx = idx\n",
    "\n",
    "                if stop_idx - start_idx > 0:\n",
    "                    correction[start_idx:stop_idx,0] += 1\n",
    "                    correction[start_idx:stop_idx,1:] += loc(phone_data[current][start_idx:stop_idx,0])                    \n",
    "\n",
    "            correction[:,1] /= correction[:,0]\n",
    "            correction[:,2] /= correction[:,0]\n",
    "            \n",
    "            corrections[current] = correction.copy()\n",
    "        \n",
    "        for phone in phone_list:\n",
    "            cond = np.logical_and(df['collectionName'] == collection[0], df['phoneName'] == phone[0]).to_list()\n",
    "            \n",
    "            df.loc[cond, ['latDeg', 'lngDeg']] = corrections[phone[0]][:,1:]            \n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0e50c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361704f7b34c4918b45775b050c3a22a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lib.kalman_filter import generate_kalmanfilter,apply_kalmanfilter\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "train_path =  \"/work/data/input/selfmade_dataset/baseline_with_derived_data_v5/train.pkl\"\n",
    "train_df = load_pickle_data(train_path);\n",
    "\n",
    "\n",
    "s_list = [];\n",
    "for key,each_df in tqdm(train_df.groupby(\"phone\")):\n",
    "    \n",
    "    tmp_df = each_df.copy()\n",
    "    kf = generate_kalmanfilter()\n",
    "    s = apply_kalmanfilter(tmp_df[[\"latDeg\",\"lngDeg\"]].to_numpy(),kf)\n",
    "    tmp_df[[\"latDeg\",\"lngDeg\"]] = s\n",
    "        \n",
    "    s_list.append(tmp_df)\n",
    "\n",
    "after_kalman = pd.concat(s_list).sort_index()\n",
    "\n",
    "after_kalman_gauss = apply_gauss_smoothing(after_kalman,{'sz_1' : 0.85, 'sz_2' : 5.65, 'sz_crit' : 1.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "260ecc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_gauss = apply_gauss_smoothing(train_df,{'sz_1' : 0.85, 'sz_2' : 5.65, 'sz_crit' : 1.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fef98b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7885c8d17c4e2282c55c95ee9042ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_list = [];\n",
    "for key,each_df in tqdm(after_gauss.groupby(\"phone\")):\n",
    "    \n",
    "    tmp_df = each_df.copy()\n",
    "    kf = generate_kalmanfilter()\n",
    "    s = apply_kalmanfilter(tmp_df[[\"latDeg\",\"lngDeg\"]].to_numpy(),kf)\n",
    "    tmp_df[[\"latDeg\",\"lngDeg\"]] = s\n",
    "        \n",
    "    s_list.append(tmp_df)\n",
    "\n",
    "afuter_gauss_kalman = pd.concat(s_list).sort_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f32d809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default 5.287859611805194\n",
      "after_gauss 4.651036920160282\n",
      "after_kalman 4.603418543273516\n",
      "after_gauss_kalman 4.618812787583824\n",
      "after_kalman_gauss 3.855470261772816\n"
     ]
    }
   ],
   "source": [
    "from external_lib.evaluation_function import evaluate_function,calc_haversine\n",
    "\n",
    "train_df[\"dist\"] = calc_haversine(train_df[\"latDeg_gt\"],train_df[\"lngDeg_gt\"],train_df[\"latDeg\"],train_df[\"lngDeg\"]);\n",
    "print(\"default\",evaluate_function(train_df,\"dist\"))\n",
    "after_gauss[\"dist\"] = calc_haversine(after_gauss[\"latDeg_gt\"],after_gauss[\"lngDeg_gt\"],after_gauss[\"latDeg\"],after_gauss[\"lngDeg\"]);\n",
    "print(\"after_gauss\",evaluate_function(after_gauss,\"dist\"))\n",
    "after_kalman[\"dist\"] = calc_haversine(after_kalman[\"latDeg_gt\"],after_kalman[\"lngDeg_gt\"],after_kalman[\"latDeg\"],after_kalman[\"lngDeg\"]);\n",
    "print(\"after_kalman\",evaluate_function(after_kalman,\"dist\"))\n",
    "\n",
    "\n",
    "afuter_gauss_kalman[\"dist\"] = calc_haversine(afuter_gauss_kalman[\"latDeg_gt\"],afuter_gauss_kalman[\"lngDeg_gt\"],afuter_gauss_kalman[\"latDeg\"],afuter_gauss_kalman[\"lngDeg\"]);\n",
    "print(\"after_gauss_kalman\",evaluate_function(afuter_gauss_kalman,\"dist\"))\n",
    "\n",
    "#s[\"dist\"] = calc_haversine(s[\"latDeg_gt\"],s[\"lngDeg_gt\"],s[\"latDeg\"],s[\"lngDeg\"]);\n",
    "#print(\"after_kalman_gauss\",evaluate_function(s,\"dist\"))\n",
    "\n",
    "\n",
    "c = mean_with_other_phones(afuter_gauss_kalman.copy())\n",
    "c[\"dist\"] = calc_haversine(c[\"latDeg_gt\"],c[\"lngDeg_gt\"],c[\"latDeg\"],c[\"lngDeg\"]);\n",
    "print(\"after_kalman_gauss\",evaluate_function(c,\"dist\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9f5d0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "latDeg    0\n",
       "lngDeg    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[[\"latDeg\",\"lngDeg\"]].isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
