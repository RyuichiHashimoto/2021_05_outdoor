{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54701416",
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a676b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lib.io import load_pickle_data\n",
    "from lib.noglobal import noglobal\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder,MinMaxScaler\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab3529",
   "metadata": {},
   "source": [
    "## data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e2edd",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0654ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"/work/data/input/selfmade_dataset/baseline_with_derived_data_v1/train.pkl\"\n",
    "test_path = \"/work/data/input/selfmade_dataset/baseline_with_derived_data_v1/test.pkl\"\n",
    "df = load_pickle_data(train_path)\n",
    "test_df = load_pickle_data(test_path)\n",
    "\n",
    "train_df_path = ['2020-05-14-US-MTV-2', '2020-06-04-US-MTV-1', '2020-06-11-US-MTV-1', '2020-09-04-US-SF-2', '2021-01-04-US-RWC-2', '2021-03-10-US-SVL-1', '2021-04-28-US-MTV-1', '2021-04-29-US-SJC-2']\n",
    "valid_df_path = ['2020-05-14-US-MTV-1', '2020-05-21-US-MTV-1', '2020-05-21-US-MTV-2', '2020-05-29-US-MTV-1', '2020-05-29-US-MTV-2', '2020-06-05-US-MTV-1', '2020-06-05-US-MTV-2', '2020-07-08-US-MTV-1', '2020-07-17-US-MTV-1', '2020-07-17-US-MTV-2', '2020-08-03-US-MTV-1', '2020-08-06-US-MTV-2', '2020-09-04-US-SF-1', '2021-01-04-US-RWC-1', '2021-01-05-US-SVL-1', '2021-01-05-US-SVL-2', '2021-04-15-US-MTV-1', '2021-04-22-US-SJC-1', '2021-04-26-US-SVL-1', '2021-04-28-US-SJC-1', '2021-04-29-US-MTV-1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e2900",
   "metadata": {},
   "source": [
    "### 欠損値処理 (0埋め)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5dac221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(0)\n",
    "test_df = test_df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f0af79",
   "metadata": {},
   "source": [
    "### ラベル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8189dd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "derrived = [[\"svid_correctedPrM_\"+str(i),\"svid_xSatPosM_\"+str(i),\"svid_ySatPosM_\"+str(i),\"svid_zSatPosM_\"+str(i)] for i  in range(1,38) ]\n",
    "baseline_pred = [\"latDeg\",\"lngDeg\",\"heightAboveWgs84EllipsoidM\"]\n",
    "#baseline_pred = [\"latDeg\",\"lngDeg\",\"heightAboveWgs84EllipsoidM\",\"prev_lat\",\"prev_lng\",\"prev_heightAboveWgs84EllipsoidM\"]\n",
    "\n",
    "\n",
    "x_col =  baseline_pred +  sum( derrived, [])\n",
    "y_col = [\"LatDeg_gt\",\"LngDeg_gt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf28f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ad47a0",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b125b432",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/ebinan92/time-series-rnn-xy-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8723966e",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c11627",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44cb16",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8d83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,df_x,df_y,transform=None,MEMORY_LENGTH=5):\n",
    "        \n",
    "        self.transform = transform;\n",
    "        \n",
    "        self.train_x = df_x.reset_index()\n",
    "        self.train_y = df_y.reset_index()\n",
    "        self.memory_length = MEMORY_LENGTH\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        x = self.train_x.iloc[idx]\n",
    "        y = self.train_x.iloc[idx]        \n",
    "        \n",
    "        x_out = torch.tensor(x.to_numpy()).float();\n",
    "        y_out = torch.tensor(y.to_numpy()).float();\n",
    "                        \n",
    "        return x_out,y_out\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_x);\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0699b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_layer_size=152):\n",
    "        super(simple_MLP,self).__init__();\n",
    "        \n",
    "    \n",
    "        self.fc1 = nn.Linear(input_layer_size,512);\n",
    "        self.fc2 = nn.Linear(512,1024);\n",
    "        self.fc3 = nn.Linear(1024,512);\n",
    "        self.fc4 = nn.Linear(512,128);\n",
    "        self.fc5 = nn.Linear(128,2);\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.normalization1 = nn.BatchNorm1d(512)\n",
    "        self.normalization2 = nn.BatchNorm1d(1024)\n",
    "        self.normalization3 = nn.BatchNorm1d(512)\n",
    "        self.normalization4 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU(False);\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.normalization1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.normalization2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.normalization3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.normalization4(x)\n",
    "                \n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x;\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c46cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svid_correctedPrm = [ \"svid_correctedPrM_\"+str(i) for i in range(1,38)];\n",
    "svid_xSatPosM = [ \"svid_xSatPosM_\"+str(i) for i in range(1,38)];\n",
    "svid_ySatPosM = [ \"svid_xSatPosM_\"+str(i) for i in range(1,38)];\n",
    "svid_zSatPosM = [ \"svid_xSatPosM_\"+str(i) for i in range(1,38)];\n",
    "baseline_pred_lat = [\"latDeg\"]\n",
    "baseline_pred_lng = [\"lngDeg\"]\n",
    "baseline_pred_height = [\"heightAboveWgs84EllipsoidM\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b67897",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7479b1",
   "metadata": {},
   "source": [
    "### svid_correctedPrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe574c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svid_correctedPrm_encoder = StandardScaler()\n",
    "svid_correctedPrm_encoder.fit(df.loc[:,svid_correctedPrm])\n",
    "\n",
    "df.loc[:,svid_correctedPrm] = svid_correctedPrm_encoder.transform(df.loc[:,svid_correctedPrm]);\n",
    "df.loc[:,svid_correctedPrm] = svid_correctedPrm_encoder.transform(df.loc[:,svid_correctedPrm]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e2496",
   "metadata": {},
   "source": [
    "### svid_xSatPosM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "297e2648",
   "metadata": {},
   "outputs": [],
   "source": [
    "svid_xSatPosM_encoder = StandardScaler()\n",
    "svid_xSatPosM_encoder.fit(df.loc[:,svid_xSatPosM])\n",
    "\n",
    "df.loc[:,svid_xSatPosM] = svid_xSatPosM_encoder.transform(df.loc[:,svid_xSatPosM]);\n",
    "test_df.loc[:,svid_xSatPosM] = svid_xSatPosM_encoder.transform(test_df.loc[:,svid_xSatPosM]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8394c42",
   "metadata": {},
   "source": [
    "### svid_ySatPosM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a4704bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "svid_ySatPosM_encoder = StandardScaler()\n",
    "svid_ySatPosM_encoder.fit(df.loc[:,svid_ySatPosM])\n",
    "\n",
    "df.loc[:,svid_ySatPosM] = svid_ySatPosM_encoder.transform(df.loc[:,svid_ySatPosM]);\n",
    "test_df.loc[:,svid_ySatPosM] = svid_ySatPosM_encoder.transform(test_df.loc[:,svid_ySatPosM]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8a35f",
   "metadata": {},
   "source": [
    "### svid_zSatPosM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6590878",
   "metadata": {},
   "outputs": [],
   "source": [
    "svid_zSatPosM_encoder = StandardScaler()\n",
    "svid_zSatPosM_encoder.fit(df.loc[:,svid_zSatPosM])\n",
    "\n",
    "df.loc[:,svid_zSatPosM] = svid_zSatPosM_encoder.transform(df.loc[:,svid_zSatPosM]);\n",
    "test_df.loc[:,svid_zSatPosM] = svid_zSatPosM_encoder.transform(test_df.loc[:,svid_zSatPosM]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83338bc6",
   "metadata": {},
   "source": [
    "### heightAboveWgs84EllipsoidM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f4d12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred_height_encoder = StandardScaler()\n",
    "baseline_pred_height_encoder.fit(df.loc[:,baseline_pred_height])\n",
    "\n",
    "df.loc[:,baseline_pred_height] = baseline_pred_height_encoder.transform(df.loc[:,baseline_pred_height]);\n",
    "test_df.loc[:,baseline_pred_height] = baseline_pred_height_encoder.transform(test_df.loc[:,baseline_pred_height]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9309ca",
   "metadata": {},
   "source": [
    "### baseline_pred_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92380993",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred_lat_encoder = MinMaxScaler()\n",
    "baseline_pred_lat_encoder.fit(df.loc[:,baseline_pred_lat])\n",
    "\n",
    "df.loc[:,baseline_pred_lat] = baseline_pred_lat_encoder.transform(df.loc[:,baseline_pred_lat]);\n",
    "test_df.loc[:,baseline_pred_lat] = baseline_pred_lat_encoder.transform(test_df.loc[:,baseline_pred_lat]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169597d8",
   "metadata": {},
   "source": [
    "### baseline_pred_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80232103",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pred_lng_encoder = MinMaxScaler()\n",
    "baseline_pred_lng_encoder.fit(df.loc[:,baseline_pred_lng])\n",
    "\n",
    "df.loc[:,baseline_pred_lng] = baseline_pred_lng_encoder.transform(df.loc[:,baseline_pred_lng]);\n",
    "test_df.loc[:,baseline_pred_lng] = baseline_pred_lng_encoder.transform(test_df.loc[:,baseline_pred_lng]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530ec64",
   "metadata": {},
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "712850d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_haversine(predict, output):\n",
    "    \n",
    "    \n",
    "    lat1 = predict[:,0] + 36.0\n",
    "    lon1 = predict[:,1] - 123.0\n",
    "    \n",
    "    \n",
    "    lat2 = output[:,0] + 36.0\n",
    "    lon2 = output[:,1] - 123.0\n",
    "                \n",
    "    RADIUS = 6_367_000\n",
    "    lat1, lon1, lat2, lon2 = map(torch.rad2deg, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = torch.sin(dlat/2)**2 + torch.cos(lat1) * torch.cos(lat2) * torch.sin(dlon/2)**2\n",
    "    \n",
    "    a[a > 1] = 1\n",
    "    a[a < 0] = 0\n",
    "    \n",
    "    \n",
    "    dist = 2 * RADIUS * torch.arcsin(a**0.5)\n",
    "    \n",
    "    \n",
    "    return dist.sum()\n",
    "\n",
    "\n",
    "def compute_dist(oof, gt_df=None):\n",
    "        \n",
    "    if (gt_df is None):\n",
    "        FILES = glob.glob(f\"/work/data/input/google-smartphone-decimeter-challenge/train/*/*/ground_truth.csv\")\n",
    "        gt_list = [ pd.read_csv(f) for f in FILES ];\n",
    "        gt_df = pd.concat(gt_list,axis=0);\n",
    "            \n",
    "    \n",
    "    gt_df[\"phone\"] = gt_df[\"collectionName\"] + \"_\" + gt_df[\"phoneName\"]\n",
    "        \n",
    "    df = oof.merge(gt_df, on = ['phone','millisSinceGpsEpoch'])\n",
    "         \n",
    "    dst_oof = calc_haversine(df.latDeg_x,df.lngDeg_x, df.latDeg_y, df.lngDeg_y)\n",
    "    \n",
    "    scores = pd.DataFrame({'phone': df.phone,'dst': dst_oof})\n",
    "    scores_grp = scores.groupby('phone')\n",
    "    \n",
    "    d50 = scores_grp.quantile(.50).reset_index()\n",
    "    d50.columns = ['phone','q50']\n",
    "    d95 = scores_grp.quantile(.95).reset_index()\n",
    "    d95.columns = ['phone','q95']\n",
    "    \n",
    "    return (scores_grp.quantile(.50).mean() + scores_grp.quantile(.95).mean())/2, d50.merge(d95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be03e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_metric_xy_(output, y):\n",
    "    return torch.sqrt(((norms*(output - y[:,:2]))**2).sum())\n",
    "\n",
    "\n",
    "\n",
    "def comp_metric_xy(output, y, norms):\n",
    "    return torch.sqrt(((norms*(output - y[:,:2]))**2).sum())\n",
    "def weighted_mse_loss(output, y):\n",
    "    return (y[:, 2].reshape(-1,1)*(output - y[:,:2])**2).sum()\n",
    "def comp_metric_weighted_xy(output, y, norms):\n",
    "    return torch.sqrt(((y[:, 2].reshape(-1,1)*(norms * (output - y[:,:2]))**2)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd55de",
   "metadata": {},
   "source": [
    "## Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2dafbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col =  baseline_pred +  sum( derrived, [])\n",
    "y_col = [\"latDeg_gt\",\"lngDeg_gt\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_df = df[df.phone.str.contains(\"|\".join(train_df_path))].reset_index()\n",
    "valid_df = df[df.phone.str.contains(\"|\".join(valid_df_path))].reset_index()\n",
    "\n",
    "\n",
    "train_df_x = train_df[x_col]\n",
    "train_df_y = valid_df[y_col]\n",
    "\n",
    "valid_df_x = valid_df[x_col]\n",
    "valid_df_y = valid_df[y_col]\n",
    "\n",
    "\n",
    "training_loader = DataLoader(CustomDataset(train_df_x,train_df_y),batch_size=256,shuffle=False);\n",
    "validation_loader = DataLoader(CustomDataset(valid_df_x,valid_df_y),batch_size=256,shuffle=False);\n",
    "\n",
    "model = simple_MLP()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "criterion = comp_metric_xy_\n",
    "\n",
    "norm_x = np.max(df.loc[:, 'latDeg_gt'].values) - np.min(df.loc[:, 'latDeg_gt'].values)\n",
    "norm_y = np.max(df.loc[:, 'lngDeg_gt'].values) - np.min(df.loc[:, 'lngDeg_gt'].values)\n",
    "norms = torch.tensor([[norm_x, norm_y]])\n",
    "\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "080c03ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal()\n",
    "def train_manytomany(t_loader, v_loader, optimizer, criterion, model,norms):\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.6)\n",
    "    model.train()\n",
    "    \n",
    "    early_stopping = 0    \n",
    "    \n",
    "    \n",
    "    \n",
    "    loss_pred = 10000000000000\n",
    "    EPOCH = 1000\n",
    "    DEVICE = \"cpu\"\n",
    "    \n",
    "    \n",
    "    for e in range(EPOCH):        \n",
    "        for x,y in t_loader:\n",
    "            optimizer.zero_grad();\n",
    "            x, y = x.to(DEVICE).float(), y.to(DEVICE).float()\n",
    "            \n",
    "            output = model(x)\n",
    "            \n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step();\n",
    "        \n",
    "        scheduler.step();\n",
    "        \n",
    "        if (e+1) % 1 == 0:\n",
    "            train_loss = [];\n",
    "            train_size = [];\n",
    "            \n",
    "            valid_loss = [];\n",
    "            valid_sizees = [];\n",
    "            \n",
    "            total_point = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                ## train data\n",
    "                \n",
    "                for (x,y) in t_loader:\n",
    "                    x, y = x.to(DEVICE).float(), y.to(DEVICE).float()\n",
    "                    output = model(x);                                                 \n",
    "                    loss = criterion(output, y);\n",
    "                    \n",
    "                    train_loss.append(loss.item());                                                \n",
    "                    train_size.append(output.numel());\n",
    "                \n",
    "                \n",
    "                for (x, y) in v_loader:\n",
    "                    x, y = x.to(DEVICE).float(), y.to(DEVICE).float()\n",
    "                    output = model(x)\n",
    "                    \n",
    "                    \n",
    "                    loss = criterion(output, y)\n",
    "                    #loss2 = comp_metric_weighted_xy(output, y, norms)\n",
    "                    \n",
    "                    valid_loss.append(loss.item())\n",
    "                    #losses2.append(loss2.item())\n",
    "                    valid_sizees.append(output.numel())\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                \n",
    "                training_score = np.sum(train_loss)/np.sum(train_size)\n",
    "                validation_score = np.sum(valid_loss)/np.sum(valid_sizees)\n",
    "                \n",
    "                #tmp2 = np.sum(losses2)/np.sum(sizees)\n",
    "                                \n",
    "                print(f\"epoch: {e+1}, t_loss: {training_score:.5f}, validation loss: {validation_score:.5f}, lr:{scheduler.get_last_lr()[0]:.5f}\")\n",
    "                    \n",
    "                if loss_pred < validation_score:\n",
    "                    early_stopping += 1\n",
    "                else:\n",
    "                    early_stopping = 0\n",
    "                    loss_pred = validation_score\n",
    "                    model_pred = model\n",
    "                if early_stopping > 5:                        \n",
    "                    return model_pred, loss_pred    \n",
    "    return model_pred, loss_pred\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d64a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, t_loss: 221.37942, validation loss: 530.97418, lr:0.00500\n"
     ]
    }
   ],
   "source": [
    "model, loss = train_manytomany(\n",
    "        training_loader, validation_loader, optimizer, criterion, model, norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc29d27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
